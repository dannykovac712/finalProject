{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1Gx_Ll6ai4tbP3C8KEkGX6TzvRDa1e_Mi","timestamp":1765413870216},{"file_id":"https://gist.github.com/dannykovac712/a85422b944c0b71b264ce08f4aaec846#file-final-project-495-ipynb","timestamp":1765406515457}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install -U transformers datasets accelerate peft bitsandbytes safetensors"],"metadata":{"id":"E2vTyoZ9oyXm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import transformers\n","print(transformers.__version__)"],"metadata":{"id":"wJewyhmnpU0X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import files\n","\n","uploaded = files.upload()\n","print(uploaded.keys())"],"metadata":{"id":"VOBwCUWPoYcW"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yTs_cjaZl2sf"},"outputs":[],"source":["import os\n","from dataclasses import dataclass\n","from typing import Dict, List, Union\n","\n","import torch\n","from datasets import load_dataset\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    Trainer,\n","    TrainingArguments,\n","    DataCollatorForLanguageModeling,\n","    BitsAndBytesConfig,\n",")\n","from peft import LoraConfig, get_peft_model, TaskType\n","\n","\n","MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n","DATA_PATH = \"data.jsonl\"\n","OUTPUT_DIR = \"./structmath-lora-model\"\n","\n","MAX_LENGTH = 512\n","BATCH_SIZE = 1\n","NUM_EPOCHS = 3\n","LEARNING_RATE = 2e-4\n","WARMUP_STEPS = 50\n","GRAD_ACCUM_STEPS = 8\n","\n","\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","if tokenizer.pad_token is None:\n","    tokenizer.pad_token = tokenizer.eos_token\n","\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_use_double_quant=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=torch.float16,\n",")\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    MODEL_NAME,\n","    device_map=\"auto\",\n","    quantization_config=bnb_config,\n","    torch_dtype=torch.float16,\n",")\n","\n","\n","lora_config = LoraConfig(\n","    task_type=TaskType.CAUSAL_LM,\n","    r=16,\n","    lora_alpha=32,\n","    lora_dropout=0.05,\n","    bias=\"none\",\n","    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",")\n","\n","model = get_peft_model(model, lora_config)\n","model.print_trainable_parameters()\n","\n","\n","\n","raw_ds = load_dataset(\"json\", data_files=DATA_PATH, split=\"train\")\n","\n","def format_example(example):\n","    theorem_text = example[\"input\"]\n","    json_target = example[\"output\"]\n","\n","    prompt = (\n","        \"You are an assistant that extracts structured information from\"\n","        \" mathematical theorems.\\n\\n\"\n","        \"Task: Given the following theorem, output a JSON object with fields:\\n\"\n","        \"- type\\n- id\\n- name (optional)\\n- assumptions (list of strings)\\n\"\n","        \"- conclusion (string)\\n\\n\"\n","        \"Theorem:\\n\"\n","        f\"{theorem_text}\\n\\n\"\n","        \"JSON:\\n\"\n","        \"<json>\\n\"\n","    )\n","\n","    target = json_target + \"\\n</json>\"\n","\n","    return {\"prompt\": prompt, \"target\": target}\n","\n","formatted_ds = raw_ds.map(format_example, remove_columns=raw_ds.column_names)\n","\n","ds = formatted_ds.train_test_split(test_size=0.1, seed=42)\n","train_ds = ds[\"train\"]\n","val_ds = ds[\"test\"]\n","\n","\n","\n","\n","\n","def tokenize_fn(example):\n","    prompt_ids = tokenizer(\n","        example[\"prompt\"],\n","        add_special_tokens=False,\n","    )[\"input_ids\"]\n","\n","    target_ids = tokenizer(\n","        example[\"target\"],\n","        add_special_tokens=False,\n","    )[\"input_ids\"]\n","\n","    input_ids = prompt_ids + target_ids\n","\n","    labels = [-100] * len(prompt_ids) + target_ids\n","\n","    input_ids = input_ids[:MAX_LENGTH]\n","    labels = labels[:MAX_LENGTH]\n","\n","    attention_mask = [1] * len(input_ids)\n","\n","    return {\n","        \"input_ids\": input_ids,\n","        \"labels\": labels,\n","        \"attention_mask\": attention_mask,\n","    }\n","\n","tokenized_train = train_ds.map(\n","    tokenize_fn,\n","    batched=False,\n","    remove_columns=[\"prompt\", \"target\"],\n",")\n","\n","tokenized_val = val_ds.map(\n","    tokenize_fn,\n","    batched=False,\n","    remove_columns=[\"prompt\", \"target\"],\n",")\n","\n","tokenized_train.set_format(type=\"torch\")\n","tokenized_val.set_format(type=\"torch\")\n","\n","\n","data_collator = DataCollatorForLanguageModeling(\n","    tokenizer=tokenizer,\n","    mlm=False,\n",")\n","\n","\n","\n","training_args = TrainingArguments(\n","    output_dir=OUTPUT_DIR,\n","    per_device_train_batch_size=BATCH_SIZE,\n","    per_device_eval_batch_size=BATCH_SIZE,\n","    gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n","    num_train_epochs=NUM_EPOCHS,\n","    learning_rate=LEARNING_RATE,\n","    warmup_steps=WARMUP_STEPS,\n","    logging_steps=50,\n","    fp16=True,\n","    save_total_limit=3,\n","    report_to=[],\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_train,\n","    eval_dataset=tokenized_val,\n","    data_collator=data_collator,\n",")\n","\n","\n","trainer.train()\n","\n","trainer.save_model(OUTPUT_DIR)\n","tokenizer.save_pretrained(OUTPUT_DIR)\n","model.save_pretrained(OUTPUT_DIR)\n","\n"]},{"cell_type":"code","source":["model.save_pretrained(OUTPUT_DIR)\n"],"metadata":{"id":"L4KDPCzOufKl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!zip -r structmath-lora-model.zip structmath-lora-model\n"],"metadata":{"id":"fCarC-q-ujBx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import files\n","files.download(\"structmath-lora-model.zip\")"],"metadata":{"id":"ymChMgBouuD7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import zipfile\n","from google.colab import files\n","\n","LORA_ZIP_NAME = \"structmath-lora-model.zip\"\n","LORA_DIR = \"structmath-lora-model\"\n","\n","uploaded = files.upload()\n","\n","if LORA_ZIP_NAME not in uploaded:\n","    LORA_ZIP_NAME = list(uploaded.keys())[0]\n","\n","if os.path.isdir(LORA_DIR):\n","    import shutil\n","    shutil.rmtree(LORA_DIR)\n","\n","with zipfile.ZipFile(LORA_ZIP_NAME, 'r') as zf:\n","    zf.extractall(\".\")\n","\n","if not os.path.isdir(LORA_DIR):\n","    raise RuntimeError(f\" Expected directory '{LORA_DIR}' but it was not created.\")\n","\n","contents = os.listdir(LORA_DIR)"],"metadata":{"id":"ublGpw937Cc5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import re\n","import json\n","import torch\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","from peft import PeftModel\n","\n","BASE_MODEL = \"mistralai/Mistral-7B-Instruct-v0.2\"\n","LORA_DIR = \"./structmath-lora-model\"  # adjust if needed\n","\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","tokenizer = AutoTokenizer.from_pretrained(LORA_DIR)\n","if tokenizer.pad_token is None:\n","    tokenizer.pad_token = tokenizer.eos_token\n","\n","base_model = AutoModelForCausalLM.from_pretrained(\n","    BASE_MODEL,\n","    torch_dtype=torch.float16,\n","    low_cpu_mem_usage=False,\n",")\n","\n","model = PeftModel.from_pretrained(\n","    base_model,\n","    LORA_DIR,\n","    low_cpu_mem_usage=False,\n","    is_trainable=False,\n",")\n","\n","model.to(DEVICE)\n","model.eval()\n","\n","\n","def build_prompt(theorem_text: str) -> str:\n","    return (\n","        \"You are an assistant that extracts structured information from\"\n","        \" mathematical theorems.\\n\\n\"\n","        \"Task: Given the following theorem, output a JSON object with fields:\\n\"\n","        \"- type\\n- id\\n- name (optional)\\n- assumptions (list of strings)\\n\"\n","        \"- conclusion (string)\\n\\n\"\n","        \"Theorem:\\n\"\n","        f\"{theorem_text}\\n\\n\"\n","        \"JSON:\\n\"\n","        \"<json>\\n\"\n","    )\n","\n","\n","def extract_json_block(text: str):\n","    m = re.search(r\"<json>\\s*(\\{.*?\\})\\s*</json>\", text, re.S)\n","    if not m:\n","        return None\n","    return m.group(1).strip()\n","\n","\n","def test_llm(theorem_text: str):\n","    prompt = build_prompt(theorem_text)\n","    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n","\n","    with torch.no_grad():\n","        outputs = model.generate(\n","            **inputs,\n","            max_new_tokens=256,\n","            temperature=0.0,\n","            do_sample=False,\n","        )\n","\n","    full_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","    print(\"\\n=== Full model output ===\")\n","    print(full_text)\n","\n","    json_block = extract_json_block(full_text)\n","    if json_block is None:\n","        print(\"\\n No <json>...</json> block found.\")\n","        return\n","\n","    try:\n","        parsed = json.loads(json_block)\n","    except json.JSONDecodeError as e:\n","        print(\"\\n JSON parse error:\", e)\n","        print(\"Raw JSON block:\")\n","        print(json_block)\n","        return\n","\n","    print(\"\\n Parsed JSON:\")\n","    print(json.dumps(parsed, indent=2))\n"],"metadata":{"id":"EkvyOf2k11gB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_llm(\"For an elliptic differential operator on a compact manifold, its analytical index equals its topological index.\")\n","test_llm(\"Let f be differentiable on the interval (a,b). Then f is continuous on (a,b).\")\n","test_llm(\"Let T: V â†’ W be a linear transformation. If T is injective, then T(v) = 0 implies v = 0.\")\n","test_llm(\"In a group G, the identity element is unique.\")\n","test_llm(\"Let (X, d) be a metric space. If X is compact, then every sequence in X has a convergent subsequence.\")\n","\n"],"metadata":{"id":"aL697MyF9jrD"},"execution_count":null,"outputs":[]}]}